---
title: "assignment_3_COMM661_Zining"
author: "Zining Wang"
date: "11/11/2019"
output:
  html_document: default
  pdf_document: default
---
# 1. Binary Probit Model 
  - code to generate random samples for the parameters in a binary probit model 
  - Program inputs: number of iterations, dependent variable, independent variables, priors 
  - Program outputs: beta and variance matrix of beta 
  - Use simulated data to confirm the correctness of your estimation algorithm 
  - Comment on how the specification of priors would affect model estimates 

# 2. Multinominal Logit Model
  - code to generate random samples for the parameters in a multinomial logit model.
  - Program inputs: number of iterations, dependent variable, independent variables, priors
  -  Program outputs: beta and variance matrix of beta
  - Please use simulated data to confirm the correctness of your estimation algorithm
  - Please comment on the performance of mixing/autocorrelations of the output you get 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# setup
rm(list = ls())
path = '/Users/wangzining/Desktop/year_2/COMM661/assign_3'
setwd(path)
source("mvrnorm.R")
# install.packages("truncnorm")
#install.packages("emdbook")
require(emdbook)
require(truncnorm)
require(ggplot2)
require(data.table)
require(DescTools)
require(MASS)
require(bayesplot)
options(scipen=999)
```
# 1. Binary Probit Model
-Data Simulation and Model inputs

```{r}
# Data Simulation
nsim = 10000 # of simulation
N = 1000 # of obs

betas_true = c(0.3, 0.6, 0.9) # beta
#betas_true = c(-1,0.5,1.5)
# intcpt = 1 # intcpt
X = cbind(rep(1, N), mvrnorm_fct(N, c(0, 0), Sigma = diag(2)))
E = rnorm(n=N, mean=0, sd=1)
Z = X %*% betas_true+ E
Y = rep(0, N)
Y[which(Z>0)] = 1
```
-binary probit function
```{r binary_logit_fct}
binary_probit <- function(X, Z, Y, N, beta_0, B_0, nsim){
  # X: covariates
  # Z: latent variable
  # Y: Choice outcome
  # N: number of observations
  # beta_0: prior of means of parameters
  # B_0: prior of variance of parameters
  # nsim: rounds of simulations
  
  nparams=ncol(X)
  sims=matrix(0,nrow=nsim,ncol=nparams)
  sims[1,]=1 # to start
  
  #Gibbs Sampler
  for(i in 2:nsim){
    # updata beta
    beta_p = solve(t(X) %*% X + solve(B_0)) %*% (t(X)%*%Z + solve(B_0) %*% beta_0)
    # updata B
    B_p = solve(t(X) %*% X + solve(B_0))
    # update Z
    Z_p = rep(NA, N)
    mean_trunc =X%*%sims[i-1,] # X %*% beta_p
    Z_p[which(Y==1)] = rtruncnorm(1, a = 0, b=Inf, mean = mean_trunc[which(Y==1)], sd = 1)
    Z_p[which(Y==0)] = rtruncnorm(1, a = -Inf, b=0, mean = mean_trunc[which(Y==0)], sd = 1)
    Z = Z_p # assign posterior as prior
    # the updated of Z and beta is interchangable
    sims[i,1:nparams]=mvrnorm_fct(1, beta_p, B_p) # draw beta: from multivariate normal: Pi(beta|y, B) <- N_k (beta_hat, B_hat)
  }
  return(sims)
}
```
-output
```{r}
# set priors
beta_0 = c(0.01, 0.01, 0.01) # beta*, prior: beta ~ N(beta_0, B_0), uninformative prior
B_0 = diag(3) # B*, prior: beta ~ N(beta*, B_0)

output = binary_probit(X = X, Z = Z, Y = Y, N = N, beta_0 = beta_0, B_0 = B_0, nsim = nsim)
output = setDT(as.data.frame(output))

output[,draws:=seq_len(nrow(output))]
colnames(output) = c("beta_0: 0.3", "beta_1: 0.6", "beta_2: 0.9", "draws")


color_scheme_set("blue")
plot = mcmc_trace(output[,-4])
```

```{r}
plot 
```

```{r}
# change priors to different values
beta_0 = c(1000, 1000, 1000) # beta*, prior: beta ~ N(beta_0, B_0), informative but biased prior
B_0 = diag(3) # B*, prior: beta ~ N(beta*, B_0)
output = binary_probit(X = X, Z = Z, Y = Y, N = N, beta_0 = beta_0, B_0 = B_0, nsim = nsim)
output = setDT(as.data.frame(output))

output[,draws:=seq_len(nrow(output))]
colnames(output) = c("beta_0: 0.3", "beta_1: 0.6", "beta_2: 0.9", "draws")

color_scheme_set("blue")
plot = mcmc_trace(output[,-4])
```

```{r}
plot
```

- Given uninformative priors, the parameters are accurately recovered. Given strong but biased priors (which is quite different from the true values), the parameters cannot be accurately recovered.  
# 2. Multinominal Logit Model
- Data Simulation and Model inputs
```{r}
set.seed(1) # set seeds

X = matrix(rnorm(2000), 1000, 2) # 2 covairates
X = cbind(rep(1, 1000), X)
# coefficients for each choice, 3 choices altogether

Coef1 = rep(0, 3)
Coef2 = c(0.9 , 0.7, 1.5)
Coef3 = c(0.5, 2, 1.2)

# vector of probabilities
Prob = cbind(exp(X%*%Coef1), exp(X%*%Coef2), exp(X%*%Coef3))

# multinomial draws
Choices = t(apply(Prob, 1, rmultinom, n = 1, size = 1))
M = cbind.data.frame(y = apply(Choices, 1, function(x) which(x==1)), X)
table(M$y)

betas_true = c(0.9, 0.7, 1.5, 0.5, 2, 1.2) # beta, 6 parameters, 2 intercept term

mu=c(1, 1, 1, 1, 1, 1) # prior of beta (mean)
Sigma=diag(6)*1 # prior of beta (variance)
# try reg
#reg_data = cbind.data.frame(C[,I], X[,-1])
#colnames(reg_data) = c('y', 'x1', 'x2')
#rownames(reg_data) = NULL
#library("nnet")
#require(mlogit)
#summary(multinom(y ~ x1 + x2, data = reg_data))

```
- multinormial logit model
```{r}
# hession

multinorm_logit = function(X, I, beta_prev, mu, Sigma, sd_tune, nsim){ 
  # X: covariates
  # I: choice outcomes
  # beta_prev: prior of parameters
  # mu and Sigma hyper parameters (mean and variance of parameters)
  # sd_tune: tuning parameter to decite the pace of beta proposals
  # nsim: rounds of simulations
  
  nparams=length(beta_prev) # number of parameters
  sims=matrix(0, nrow=nsim,ncol=nparams) # empty matrix to store beta draws
  sims[1,] = beta_prev # to start, set prior
  
  for(i in 2:nsim){
    # draw prior of beta
    Prob_MH = function(beta){ # use random walk to get beta proposals
    prior =  dmvnorm(x = beta, mu=mu, Sigma=Sigma, log=TRUE) # set prior for each round

    # update the likelihood of chosen each alternative
    U = cbind(rep(1, N), exp(X%*%beta[1:ncol(X)]), exp(X%*%beta[(ncol(X)+1):length(beta)]))
    # U = matrix( exp(X%*% beta), nrow = N, byrow = F) # calculate the utility given the beta input
    
    likelihood = rep(NA, N)
    for (ind in 1:N){
      likelihood[ind] = log(U[ind, I[ind]]/(sum(U[ind,])))
    } # calculate the log likelihood: log(L(beta|X, I)), 
      # in which L(beta|X, I) = prod(Pr(I_i = j|X_i, beta)), 
      # Pr(I_i = j|X_i, beta) = exp(x'_ijbeta)/(summation(exp(x'_ijbeta)))
      # the log version is used for numeric stability
      # reference: Rossi, P. E., Allenby, G. M., & McCulloch, R. (2012). Bayesian statistics and marketing. John Wiley & Sons.
      # reference: https://www4.stat.ncsu.edu/~reich/ABA/code/logit
    
    posterior = prior + sum(likelihood) # use the log version (multiple changes to addition)
    return(posterior)
    }
    # use MH
    
    #for (j in seq_len(nparams)){ # iterate over all elements of beta
    #beta_post =beta_prev

    # proporal of beta[j]
    # consider multivariate student t distribution
    beta_post = beta_prev + mvrnorm(1, mu=rep(0, 6), Sigma=diag(6)*sd_tune^2) # can be tightened/loosened
    # beta_post = beta_prev + mvrnorm(1, mu=c(0, 0, 0), Sigma=diag(3)*sd_tune^2) # can be tightened/loosened
    
    # probability of move
    p_accept = min(1, exp(Prob_MH(beta_post) - Prob_MH(beta_prev)) ) # use exp transformation, use "-" instead of "/" to avoid rounding to 0
    
    # decite whether to accept or to reject:
    # draw from a uniform distribution [0,1]
    draw = runif(1, 0, 1)
    if (draw <= p_accept){beta_prev = beta_post}else{beta_post = beta_prev}
    #}
    sims[i,] = beta_post # store the updated params
  }
return(sims)
}

```

- change the tuning parameter (sd to generate proposals of beta) 

```{r}
output_1 = multinorm_logit(X=X, I=M$y, beta_prev = c(0.01, 0.01, 0.01, 0.01, 0.01, 0.01), mu = mu, Sigma = Sigma, sd_tune = 0.05, nsim =10000)

output_1 = setDT(as.data.frame(output_1))
output_1[,draws:=seq_len(nrow(output_1))]
colnames(output_1) = c("beta_1: 0.9", "beta_2: 0.7", "beta_3: 1.5","beta_4: 0.5", "beta_5: 2", "beta_6: 1.2", "draws")
mean_output = apply(output_1, 2, mean)
mean_output
color_scheme_set("blue")
plot1 = mcmc_trace(output_1[,-7])

# betas_true = c(0.9, 0.7, 1.5, 0.5, 2, 1.2) 
# calculate lags
output_1_lag_10 = output_1[-c(1:10), ]
test.1.10 = c(cor(output_1_lag_10[["beta_1: 0.9"]], output_1[["beta_1: 0.9"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_2: 0.7"]], output_1[["beta_2: 0.7"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_3: 1.5"]], output_1[["beta_3: 1.5"]][1:(nrow(output_1)-10)]),
              cor(output_1_lag_10[["beta_4: 0.5"]], output_1[["beta_4: 0.5"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_5: 2"]], output_1[["beta_5: 2"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_6: 1.2"]], output_1[["beta_6: 1.2"]][1:(nrow(output_1)-10)]))
test.1.10

output_1_lag_100 = output_1[-c(1:100), ]
test.1.100 =c(cor(output_1_lag_100[["beta_1: 0.9"]], output_1[["beta_1: 0.9"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_2: 0.7"]], output_1[["beta_2: 0.7"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_3: 1.5"]], output_1[["beta_3: 1.5"]][1:(nrow(output_1)-100)]),
              cor(output_1_lag_100[["beta_4: 0.5"]], output_1[["beta_4: 0.5"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_5: 2"]], output_1[["beta_5: 2"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_6: 1.2"]], output_1[["beta_6: 1.2"]][1:(nrow(output_1)-100)]))
test.1.100

#output_1_melt = melt(output_1, id.vars = "draws",  measure.vars = c("beta_1: 0.9", "beta_2: 0.7", "beta_3: 1.5","beta_4: 0.5", "beta_5: 2", "beta_6: 1.2")) 
#p_posterior_0.1 <- ggplot(output_1_melt, aes( x =draws , y = value ) ) +geom_point()  + facet_wrap(vars(variable), ncol=3) + ggtitle("Posteriors: Multinomial Logit, sd = 0.1") 

```

```{r}
plot1 
```

```{r}

output_1 = multinorm_logit(X=X, I=M$y, beta_prev = c(0.01, 0.01, 0.01, 0.01, 0.01, 0.01), mu = mu, Sigma = Sigma, sd_tune = 0.02, nsim =10000)

output_1 = setDT(as.data.frame(output_1))
output_1[,draws:=seq_len(nrow(output_1))]
colnames(output_1) = c("beta_1: 0.9", "beta_2: 0.7", "beta_3: 1.5","beta_4: 0.5", "beta_5: 2", "beta_6: 1.2", "draws")
mean_output = apply(output_1, 2, mean)
mean_output
color_scheme_set("blue")
plot_02 = mcmc_trace(output_1[,-7])

# betas_true = c(0.9, 0.7, 1.5, 0.5, 2, 1.2) 
# calculate lags
output_1_lag_10 = output_1[-c(1:10), ]
test.1.10 = c(cor(output_1_lag_10[["beta_1: 0.9"]], output_1[["beta_1: 0.9"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_2: 0.7"]], output_1[["beta_2: 0.7"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_3: 1.5"]], output_1[["beta_3: 1.5"]][1:(nrow(output_1)-10)]),
              cor(output_1_lag_10[["beta_4: 0.5"]], output_1[["beta_4: 0.5"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_5: 2"]], output_1[["beta_5: 2"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_6: 1.2"]], output_1[["beta_6: 1.2"]][1:(nrow(output_1)-10)]))
test.1.10

output_1_lag_100 = output_1[-c(1:100), ]
test.1.100 =c(cor(output_1_lag_100[["beta_1: 0.9"]], output_1[["beta_1: 0.9"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_2: 0.7"]], output_1[["beta_2: 0.7"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_3: 1.5"]], output_1[["beta_3: 1.5"]][1:(nrow(output_1)-100)]),
              cor(output_1_lag_100[["beta_4: 0.5"]], output_1[["beta_4: 0.5"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_5: 2"]], output_1[["beta_5: 2"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_6: 1.2"]], output_1[["beta_6: 1.2"]][1:(nrow(output_1)-100)]))
test.1.100
```

```{r}
plot_02
```

```{r}
output_1 = multinorm_logit(X=X, I=M$y, beta_prev = c(0.01, 0.01, 0.01, 0.01, 0.01, 0.01), mu = mu, Sigma = Sigma, sd_tune = 0.2, nsim =10000)

output_1 = setDT(as.data.frame(output_1))
output_1[,draws:=seq_len(nrow(output_1))]
colnames(output_1) = c("beta_1: 0.9", "beta_2: 0.7", "beta_3: 1.5","beta_4: 0.5", "beta_5: 2", "beta_6: 1.2", "draws")
mean_output = apply(output_1, 2, mean)
mean_output
color_scheme_set("blue")
plot_02 = mcmc_trace(output_1[,-7])

# betas_true = c(0.9, 0.7, 1.5, 0.5, 2, 1.2) 
# calculate lags
output_1_lag_10 = output_1[-c(1:10), ]
test.1.10 = c(cor(output_1_lag_10[["beta_1: 0.9"]], output_1[["beta_1: 0.9"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_2: 0.7"]], output_1[["beta_2: 0.7"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_3: 1.5"]], output_1[["beta_3: 1.5"]][1:(nrow(output_1)-10)]),
              cor(output_1_lag_10[["beta_4: 0.5"]], output_1[["beta_4: 0.5"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_5: 2"]], output_1[["beta_5: 2"]][1:(nrow(output_1)-10)]), cor(output_1_lag_10[["beta_6: 1.2"]], output_1[["beta_6: 1.2"]][1:(nrow(output_1)-10)]))
test.1.10

output_1_lag_100 = output_1[-c(1:100), ]
test.1.100 =c(cor(output_1_lag_100[["beta_1: 0.9"]], output_1[["beta_1: 0.9"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_2: 0.7"]], output_1[["beta_2: 0.7"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_3: 1.5"]], output_1[["beta_3: 1.5"]][1:(nrow(output_1)-100)]),
              cor(output_1_lag_100[["beta_4: 0.5"]], output_1[["beta_4: 0.5"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_5: 2"]], output_1[["beta_5: 2"]][1:(nrow(output_1)-100)]), cor(output_1_lag_100[["beta_6: 1.2"]], output_1[["beta_6: 1.2"]][1:(nrow(output_1)-100)]))
test.1.100
```

```{r}
plot_02
```

- When sd is set to be a small value(e.g. 0.02), the beta draws slowly appoaches to the true values; when sd is set to be a larger value(e.g. 0.2), the beta draws appoaches to the true values faster. The scope of difference acoss beta draws in later rounds is also larger. Hence, the results suggest there is a trade-off between converge rate and precision.
